{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[Curator]  SSL + Image Similarity Search + Active Labeler",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spaceml-org/Curator-Unlabeled-Image-Search-Guide/blob/main/notebooks/SSL%2BImage_Similarity_Search%2BActive_Labeler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXXtv9tx1XG9"
      },
      "source": [
        "In this demo, we'll demonstrate how to \n",
        "1. train a model with Self-Supervised Learner (SSL)\n",
        "2. find similar images with Image Similarity Search\n",
        "3. improve our model with Active Labeler\n",
        "\n",
        "[Notice]\n",
        "*   Image Similarity Search and Swipe Labeler operate on your local computer, not in Colab notebook.\n",
        "*   We used [UC Merced Land Use dataset](https://weegee.vision.ucmerced.edu/datasets/landuse.html) in this demo. Although UC Merced dataset has labels, we set up the dataset as if it is unlabeled dataset to demonstrate how to use unlabeled dataset in this pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6GySXQMwYU"
      },
      "source": [
        "# 1. Self-Supervised Learner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt2c8BzpBOhm"
      },
      "source": [
        "## 1-1. Install packages & SSL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sveuC0AmV6Ye",
        "outputId": "851d81ce-7c33-43ee-b23f-13d9f12fe842"
      },
      "source": [
        "#installs\n",
        "!pip install -q split-folders\n",
        "!pip install -q torch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 torchtext==0.6.0\n",
        "!pip install -q pytorch-lightning==1.1.8\n",
        "!pip install -q pytorch-lightning-bolts\n",
        "!pip install -q --extra-index-url https://developer.download.nvidia.com/compute/redist nvidia-dali-cuda100\n",
        "!pip install -q wandb\n",
        "!pip install -q annoy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 776.8 MB 18 kB/s \n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 24 kB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 36.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 36.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 696 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 33.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 269 kB 45.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 119 kB 48.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 39.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 294 kB 39.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 142 kB 46.7 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 253 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 282 kB 37.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 391.8 MB 16 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 45.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 35.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 646 kB 5.3 MB/s \n",
            "\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbA8OuH_LwYx"
      },
      "source": [
        "import os\n",
        "import itertools\n",
        "import shutil\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import PIL.Image as Image\n",
        "import time\n",
        "\n",
        "#not logging on wandb in this demo\n",
        "os.environ['WANDB_MODE']='disabled'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBnX3jsXrAOX"
      },
      "source": [
        "#additional imports for Active Labeler\n",
        "\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "from imutils import paths\n",
        "import shutil\n",
        "from torchvision.datasets import ImageFolder\n",
        "from shutil import copyfile\n",
        "import random \n",
        "\n",
        "\n",
        "class_chosen = \"island\"\n",
        "seed = 100\n",
        "\n",
        "random.seed(seed)\n",
        "import pandas as pd\n",
        "import sys\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtYcOwoyLwYy",
        "outputId": "f5a0daf5-d4bb-4a92-e63e-55c7b2ce4113"
      },
      "source": [
        "!rm -rf SSL\n",
        "!git clone --branch simsiam https://github.com/spaceml-org/Self-Supervised-Learner.git\n",
        "!mv Self-Supervised-Learner SSL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Self-Supervised-Learner'...\n",
            "remote: Enumerating objects: 2817, done.\u001b[K\n",
            "remote: Counting objects: 100% (290/290), done.\u001b[K\n",
            "remote: Compressing objects: 100% (269/269), done.\u001b[K\n",
            "remote: Total 2817 (delta 175), reused 38 (delta 21), pack-reused 2527\u001b[K\n",
            "Receiving objects: 100% (2817/2817), 11.95 MiB | 26.04 MiB/s, done.\n",
            "Resolving deltas: 100% (1765/1765), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCpE5kcu5l0p"
      },
      "source": [
        "## 1-2. Preparing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bj4Xey4Mxk_"
      },
      "source": [
        "Before starting training on Self-Supervised Learner, we should make sure if the data is in below folder structure:\n",
        "```\n",
        "/Dataset\n",
        "    /Class 1\n",
        "        Image1.png\n",
        "        Image2.png\n",
        "    /Class 2\n",
        "        Image3.png\n",
        "        Image4.png\n",
        "```\n",
        "In case there is no label, organize directories like this:\n",
        "```\n",
        "/Dataset\n",
        "    /Unlabelled\n",
        "        Image1.png\n",
        "        Image2.png\n",
        "        Image3.png\n",
        "        Image4.png\n",
        "```\n",
        "\n",
        "UC Merced Land Use dataset is organized as the former. However, in this demo, we'll change the folder structure into the latter to treat the dataset as an unlabeled dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGbxqo7W36fl",
        "outputId": "c1b07c49-c99f-4070-9384-3646d0bb60e8"
      },
      "source": [
        "#download UC Merced Land Use dataset\n",
        "!gdown http://weegee.vision.ucmerced.edu/datasets/UCMerced_LandUse.zip\n",
        "!unzip -qq UCMerced_LandUse.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: http://weegee.vision.ucmerced.edu/datasets/UCMerced_LandUse.zip\n",
            "To: /content/UCMerced_LandUse.zip\n",
            "100% 332M/332M [00:07<00:00, 45.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efrnaKkTZpF6"
      },
      "source": [
        "#convert from tif to jpg (.tif file is not available in Swipe Labeler and Active Labeler)\n",
        "for img in list(paths.list_images('/content/UCMerced_LandUse/Images')):\n",
        "  im = Image.open(img).convert('RGB').save(img.split('.')[0] + '.jpg', \"JPEG\", quality = 100)\n",
        "  os.remove(img)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlE10H9T_nzp"
      },
      "source": [
        "#create an unlabeled image folder and copy all UC Merced dataset images into that folder\n",
        "folder= '/content/Dataset/Unlabeled'\n",
        "if os.path.exists(folder):\n",
        "    shutil.rmtree(folder)\n",
        "pathlib.Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "for i in paths.list_images('/content/UCMerced_LandUse/Images'):\n",
        "  shutil.copy(i,os.path.join(folder,i.split('/')[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkrBgq9KNKdd"
      },
      "source": [
        "## 1-3. Training self-supervised learning model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEl7UCcMMyRq",
        "outputId": "32f13b71-c57b-4db2-a402-c4c6e06c4d9e"
      },
      "source": [
        "#run this cell to check information regarding arguments\n",
        "!python /content/SSL/train.py --help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: train.py [-h] [--DATA_PATH DATA_PATH] [--VAL_PATH VAL_PATH]\n",
            "                [--model MODEL] [--batch_size BATCH_SIZE] [--cpus CPUS]\n",
            "                [--hidden_dim HIDDEN_DIM] [--epochs EPOCHS]\n",
            "                [--learning_rate LEARNING_RATE] [--patience PATIENCE]\n",
            "                [--val_split VAL_SPLIT] [--withhold_split WITHHOLD_SPLIT]\n",
            "                [--gpus GPUS] [--log_name LOG_NAME] [--image_size IMAGE_SIZE]\n",
            "                [--resize RESIZE] [--technique TECHNIQUE] [--seed SEED]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --DATA_PATH DATA_PATH\n",
            "                        path to folders with images to train on.\n",
            "  --VAL_PATH VAL_PATH   path to validation folders with images\n",
            "  --model MODEL         model to initialize. Can accept model checkpoint or\n",
            "                        just encoder name from models.py\n",
            "  --batch_size BATCH_SIZE\n",
            "                        batch size for SSL\n",
            "  --cpus CPUS           number of cpus to use to fetch data\n",
            "  --hidden_dim HIDDEN_DIM\n",
            "                        hidden dimensions in projection head or classification\n",
            "                        layer for finetuning\n",
            "  --epochs EPOCHS       number of epochs to train model\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        learning rate for encoder\n",
            "  --patience PATIENCE   automatically cuts off training if validation does not\n",
            "                        drop for (patience) epochs. Leave blank to have no\n",
            "                        validation based early stopping.\n",
            "  --val_split VAL_SPLIT\n",
            "                        percent in validation data. Ignored if VAL_PATH\n",
            "                        specified\n",
            "  --withhold_split WITHHOLD_SPLIT\n",
            "                        decimal from 0-1 representing how much of the training\n",
            "                        data to withold from either training or validation.\n",
            "                        Used for experimenting with labels neeeded\n",
            "  --gpus GPUS           number of gpus to use for training\n",
            "  --log_name LOG_NAME   name of model to log on wandb and locally\n",
            "  --image_size IMAGE_SIZE\n",
            "                        height of square image\n",
            "  --resize RESIZE       Pre-Resize data to right shape to reduce cuda memory\n",
            "                        requirements of reading large images\n",
            "  --technique TECHNIQUE\n",
            "                        SIMCLR, SIMSIAM or CLASSIFIER\n",
            "  --seed SEED           random seed for run for reproducibility\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNfn2sWSNXoP",
        "outputId": "753063b4-935e-4c01-87ed-3bfd15cc2144"
      },
      "source": [
        "#train an encoder\n",
        "!python /content/SSL/train.py --technique SIMCLR --DATA_PATH /content/Dataset --model minicnn32 --batch_size 32 --learning_rate 1e-3 --log_name ssl --image_size 256 --epochs 50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mAutomatically splitting data into train and validation data...\u001b[0m\n",
            "Copying files: 2100 files [00:00, 3737.06 files/s]\n",
            "warmup\n",
            "\u001b[34mModel architecture successfully loaded\u001b[0m\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.7/dist-packages/nvidia/dali/ops.py:627: DeprecationWarning: WARNING: `file_reader` is now deprecated. Use `readers.file` instead.\n",
            "In DALI 1.0 all readers were moved into a dedicated :mod:`~nvidia.dali.fn.readers`\n",
            "submodule and renamed to follow a common pattern. This is a placeholder operator with identical\n",
            "functionality to allow for backward compatibility.\n",
            "  op_instances.append(_OperatorInstance(input_set, self, **kwargs))\n",
            "/usr/local/lib/python3.7/dist-packages/nvidia/dali/ops.py:627: DeprecationWarning: WARNING: `image_decoder` is now deprecated. Use `decoders.image` instead.\n",
            "In DALI 1.0 all decoders were moved into a dedicated :mod:`~nvidia.dali.fn.decoders`\n",
            "submodule and renamed to follow a common pattern. This is a placeholder operator with identical\n",
            "functionality to allow for backward compatibility.\n",
            "  op_instances.append(_OperatorInstance(input_set, self, **kwargs))\n",
            "read 1680 files from 1 directories\n",
            "/usr/local/lib/python3.7/dist-packages/nvidia/dali/ops.py:627: DeprecationWarning: WARNING: `file_reader` is now deprecated. Use `readers.file` instead.\n",
            "In DALI 1.0 all readers were moved into a dedicated :mod:`~nvidia.dali.fn.readers`\n",
            "submodule and renamed to follow a common pattern. This is a placeholder operator with identical\n",
            "functionality to allow for backward compatibility.\n",
            "  op_instances.append(_OperatorInstance(input_set, self, **kwargs))\n",
            "/usr/local/lib/python3.7/dist-packages/nvidia/dali/ops.py:627: DeprecationWarning: WARNING: `image_decoder` is now deprecated. Use `decoders.image` instead.\n",
            "In DALI 1.0 all decoders were moved into a dedicated :mod:`~nvidia.dali.fn.decoders`\n",
            "submodule and renamed to follow a common pattern. This is a placeholder operator with identical\n",
            "functionality to allow for backward compatibility.\n",
            "  op_instances.append(_OperatorInstance(input_set, self, **kwargs))\n",
            "read 420 files from 1 directories\n",
            "\n",
            "  | Name       | Type       | Params\n",
            "------------------------------------------\n",
            "0 | projection | Projection | 20.9 K\n",
            "1 | encoder    | miniCNN    | 128 K \n",
            "------------------------------------------\n",
            "149 K     Trainable params\n",
            "0         Non-trainable params\n",
            "149 K     Total params\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/step_result.py:145: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  value = torch.tensor(value, device=device, dtype=torch.float)\n",
            "Epoch 0:  92% 60/65 [00:06<00:00,  9.40it/s, loss=2.93, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 65/65 [00:07<00:00,  9.13it/s, loss=2.45, v_num=vmiR]\n",
            "Epoch 1:  92% 60/65 [00:06<00:00,  9.54it/s, loss=1.89, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 65/65 [00:07<00:00,  9.25it/s, loss=1.85, v_num=vmiR]\n",
            "Epoch 2:  92% 60/65 [00:06<00:00,  9.51it/s, loss=1.82, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 65/65 [00:07<00:00,  9.24it/s, loss=1.81, v_num=vmiR]\n",
            "Epoch 3:  92% 60/65 [00:06<00:00,  9.55it/s, loss=1.77, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 65/65 [00:07<00:00,  9.28it/s, loss=1.72, v_num=vmiR]\n",
            "Epoch 4:  92% 60/65 [00:06<00:00,  9.46it/s, loss=1.73, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 65/65 [00:07<00:00,  9.21it/s, loss=1.78, v_num=vmiR]\n",
            "Epoch 5:  92% 60/65 [00:06<00:00,  9.52it/s, loss=1.49, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 65/65 [00:07<00:00,  9.24it/s, loss=1.47, v_num=vmiR]\n",
            "Epoch 6:  92% 60/65 [00:06<00:00,  9.50it/s, loss=1.62, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 65/65 [00:07<00:00,  9.23it/s, loss=1.62, v_num=vmiR]\n",
            "Epoch 7:  92% 60/65 [00:06<00:00,  9.53it/s, loss=1.56, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100% 65/65 [00:06<00:00,  9.29it/s, loss=1.55, v_num=vmiR]\n",
            "Epoch 8:  92% 60/65 [00:06<00:00,  9.54it/s, loss=1.4, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 65/65 [00:07<00:00,  9.28it/s, loss=1.32, v_num=vmiR]\n",
            "Epoch 9:  92% 60/65 [00:06<00:00,  9.57it/s, loss=1.37, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100% 65/65 [00:07<00:00,  9.27it/s, loss=1.3, v_num=vmiR] \n",
            "Epoch 10:  92% 60/65 [00:06<00:00,  9.59it/s, loss=1.33, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100% 65/65 [00:06<00:00,  9.31it/s, loss=1.35, v_num=vmiR]\n",
            "Epoch 11:  92% 60/65 [00:06<00:00,  9.60it/s, loss=1.34, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100% 65/65 [00:06<00:00,  9.34it/s, loss=1.33, v_num=vmiR]\n",
            "Epoch 12:  92% 60/65 [00:06<00:00,  9.60it/s, loss=1.33, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: 100% 65/65 [00:06<00:00,  9.32it/s, loss=1.31, v_num=vmiR]\n",
            "Epoch 13:  92% 60/65 [00:06<00:00,  9.58it/s, loss=1.33, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13: 100% 65/65 [00:06<00:00,  9.34it/s, loss=1.34, v_num=vmiR]\n",
            "Epoch 14:  92% 60/65 [00:06<00:00,  9.61it/s, loss=1.35, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14: 100% 65/65 [00:06<00:00,  9.35it/s, loss=1.34, v_num=vmiR]\n",
            "Epoch 15:  92% 60/65 [00:06<00:00,  9.55it/s, loss=1.36, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15: 100% 65/65 [00:06<00:00,  9.30it/s, loss=1.34, v_num=vmiR]\n",
            "Epoch 16:  92% 60/65 [00:06<00:00,  9.60it/s, loss=1.41, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16: 100% 65/65 [00:06<00:00,  9.33it/s, loss=1.43, v_num=vmiR]\n",
            "Epoch 17:  92% 60/65 [00:06<00:00,  9.57it/s, loss=1.31, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17: 100% 65/65 [00:06<00:00,  9.31it/s, loss=1.21, v_num=vmiR]\n",
            "Epoch 18:  92% 60/65 [00:06<00:00,  9.63it/s, loss=1.3, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18: 100% 65/65 [00:06<00:00,  9.36it/s, loss=1.28, v_num=vmiR]\n",
            "Epoch 19:  92% 60/65 [00:06<00:00,  9.57it/s, loss=1.2, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19: 100% 65/65 [00:06<00:00,  9.33it/s, loss=1.24, v_num=vmiR]\n",
            "Epoch 20:  92% 60/65 [00:06<00:00,  9.46it/s, loss=1.26, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20: 100% 65/65 [00:07<00:00,  9.24it/s, loss=1.31, v_num=vmiR]\n",
            "Epoch 21:  92% 60/65 [00:06<00:00,  9.64it/s, loss=1.32, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21: 100% 65/65 [00:06<00:00,  9.36it/s, loss=1.24, v_num=vmiR]\n",
            "Epoch 22:  92% 60/65 [00:06<00:00,  9.55it/s, loss=1.18, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22: 100% 65/65 [00:06<00:00,  9.30it/s, loss=1.21, v_num=vmiR]\n",
            "Epoch 23:  92% 60/65 [00:06<00:00,  9.57it/s, loss=1.24, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23: 100% 65/65 [00:07<00:00,  9.28it/s, loss=1.25, v_num=vmiR]\n",
            "Epoch 24:  92% 60/65 [00:06<00:00,  9.56it/s, loss=1.19, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24: 100% 65/65 [00:06<00:00,  9.29it/s, loss=1.18, v_num=vmiR]\n",
            "Epoch 25:  92% 60/65 [00:06<00:00,  9.62it/s, loss=1.21, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25: 100% 65/65 [00:06<00:00,  9.38it/s, loss=1.16, v_num=vmiR]\n",
            "Epoch 26:  92% 60/65 [00:06<00:00,  9.56it/s, loss=1.23, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26: 100% 65/65 [00:06<00:00,  9.32it/s, loss=1.2, v_num=vmiR] \n",
            "Epoch 27:  92% 60/65 [00:06<00:00,  9.61it/s, loss=1.19, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27: 100% 65/65 [00:06<00:00,  9.36it/s, loss=1.16, v_num=vmiR]\n",
            "Epoch 28:  92% 60/65 [00:06<00:00,  9.59it/s, loss=1.18, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28: 100% 65/65 [00:06<00:00,  9.30it/s, loss=1.14, v_num=vmiR]\n",
            "Epoch 29:  92% 60/65 [00:06<00:00,  9.57it/s, loss=1.16, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29: 100% 65/65 [00:06<00:00,  9.33it/s, loss=1.15, v_num=vmiR]\n",
            "Epoch 30:  92% 60/65 [00:06<00:00,  9.62it/s, loss=1.15, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30: 100% 65/65 [00:06<00:00,  9.34it/s, loss=1.14, v_num=vmiR]\n",
            "Epoch 31:  92% 60/65 [00:06<00:00,  9.61it/s, loss=1.16, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31: 100% 65/65 [00:06<00:00,  9.38it/s, loss=1.15, v_num=vmiR]\n",
            "Epoch 32:  92% 60/65 [00:06<00:00,  9.62it/s, loss=1.15, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32: 100% 65/65 [00:06<00:00,  9.34it/s, loss=1.15, v_num=vmiR]\n",
            "Epoch 33:  92% 60/65 [00:06<00:00,  9.57it/s, loss=1.19, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33: 100% 65/65 [00:06<00:00,  9.32it/s, loss=1.21, v_num=vmiR]\n",
            "Epoch 34:  92% 60/65 [00:06<00:00,  9.54it/s, loss=1.2, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34: 100% 65/65 [00:06<00:00,  9.32it/s, loss=1.2, v_num=vmiR]\n",
            "Epoch 35:  92% 60/65 [00:06<00:00,  9.55it/s, loss=1.18, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35: 100% 65/65 [00:06<00:00,  9.30it/s, loss=1.18, v_num=vmiR]\n",
            "Epoch 36:  92% 60/65 [00:06<00:00,  9.57it/s, loss=1.15, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36: 100% 65/65 [00:06<00:00,  9.32it/s, loss=1.17, v_num=vmiR]\n",
            "Epoch 37:  92% 60/65 [00:06<00:00,  9.54it/s, loss=1.13, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37: 100% 65/65 [00:07<00:00,  9.28it/s, loss=1.12, v_num=vmiR]\n",
            "Epoch 38:  92% 60/65 [00:06<00:00,  9.58it/s, loss=1.15, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38: 100% 65/65 [00:06<00:00,  9.34it/s, loss=1.13, v_num=vmiR]\n",
            "Epoch 39:  92% 60/65 [00:06<00:00,  9.65it/s, loss=1.14, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39: 100% 65/65 [00:06<00:00,  9.41it/s, loss=1.15, v_num=vmiR]\n",
            "Epoch 40:  92% 60/65 [00:06<00:00,  9.57it/s, loss=1.15, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40: 100% 65/65 [00:06<00:00,  9.29it/s, loss=1.12, v_num=vmiR]\n",
            "Epoch 41:  92% 60/65 [00:06<00:00,  9.59it/s, loss=1.11, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41: 100% 65/65 [00:06<00:00,  9.33it/s, loss=1.14, v_num=vmiR]\n",
            "Epoch 42:  92% 60/65 [00:06<00:00,  9.46it/s, loss=1.09, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42: 100% 65/65 [00:07<00:00,  9.20it/s, loss=1.08, v_num=vmiR]\n",
            "Epoch 43:  92% 60/65 [00:06<00:00,  9.67it/s, loss=1.17, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43: 100% 65/65 [00:06<00:00,  9.40it/s, loss=1.14, v_num=vmiR]\n",
            "Epoch 44:  92% 60/65 [00:06<00:00,  9.60it/s, loss=1.11, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44: 100% 65/65 [00:06<00:00,  9.36it/s, loss=1.15, v_num=vmiR]\n",
            "Epoch 45:  92% 60/65 [00:06<00:00,  9.57it/s, loss=1.14, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45: 100% 65/65 [00:06<00:00,  9.33it/s, loss=1.15, v_num=vmiR]\n",
            "Epoch 46:  92% 60/65 [00:06<00:00,  9.55it/s, loss=1.16, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46: 100% 65/65 [00:07<00:00,  9.28it/s, loss=1.16, v_num=vmiR]\n",
            "Epoch 47:  92% 60/65 [00:06<00:00,  9.61it/s, loss=1.12, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47: 100% 65/65 [00:06<00:00,  9.35it/s, loss=1.12, v_num=vmiR]\n",
            "Epoch 48:  92% 60/65 [00:06<00:00,  9.59it/s, loss=1.14, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48: 100% 65/65 [00:06<00:00,  9.34it/s, loss=1.13, v_num=vmiR]\n",
            "Epoch 49:  92% 60/65 [00:06<00:00,  9.54it/s, loss=1.14, v_num=vmiR]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49: 100% 65/65 [00:07<00:00,  9.28it/s, loss=1.15, v_num=vmiR]\n",
            "Epoch 49: 100% 65/65 [00:07<00:00,  9.27it/s, loss=1.15, v_num=vmiR]\n",
            "\u001b[34mYOUR MODEL CAN BE ACCESSED AT: \u001b[0m ./models/SIMCLR_ssl.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBe4krtNzVQg",
        "outputId": "a4d227c9-d7f7-4853-da95-38bdda5c042b"
      },
      "source": [
        "# Load model\n",
        "%cd /content/SSL\n",
        "from models import SIMCLR, SIMSIAM, CLASSIFIER\n",
        "%cd /content/\n",
        "\n",
        "model = SIMCLR.SIMCLR.load_from_checkpoint('/content/models/SIMCLR_ssl.ckpt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/SSL\n",
            "/content\n",
            "warmup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VkoTCbR_WEZ"
      },
      "source": [
        "# 2. Image Similarity Search (prep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGLVumwBSsHP"
      },
      "source": [
        "## 2-1. Download dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSxpPArsTiF0"
      },
      "source": [
        "Downloading multiple files or folders from Colab notebook to your computer can take a long time. We recommend you download 'UCMerced_LandUse.zip' to your computer and unzip it.\n",
        "\n",
        "You could download 'UCMerced_LandUse.zip' file if you only want to use Image Similarity Search. But if you also want to use Swipe Labeler or Active Labeler, run below code cell and download 'UCMerced_LandUse_jpg_ver.zip' file because they don't accept .tif format image files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtRYlApeZeIZ",
        "outputId": "ac2376c1-b726-4c37-cb48-7e1991b1b20f"
      },
      "source": [
        "!zip -r UCMerced_LandUse_jpg_ver.zip /content/Dataset/Unlabeled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/Dataset/Unlabeled/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FiI3hB_Sw0g"
      },
      "source": [
        "## 2-2. Download model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG50E9m3MjNh"
      },
      "source": [
        "To use Image Similarity Search app, we need a model file in either .pt or .pth format. Because SSL model is .ckpt format in default, we'll change the model into .pt format file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgOHozopdltb",
        "outputId": "bd57c906-e6ed-4650-bf0f-a6600f4cb363"
      },
      "source": [
        "# check torch size\n",
        "model.local_rank = 0\n",
        "model.setup(stage = 'inference') #we set up inference with this call to instantiate the DALI data pipeline\n",
        "model.eval()\n",
        "model.cuda()\n",
        "\n",
        "for batch in model.inference_dataloader:\n",
        "    print(len(batch))\n",
        "    print(batch[0].shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nvidia/dali/ops.py:627: DeprecationWarning: WARNING: `file_reader` is now deprecated. Use `readers.file` instead.\n",
            "In DALI 1.0 all readers were moved into a dedicated :mod:`~nvidia.dali.fn.readers`\n",
            "submodule and renamed to follow a common pattern. This is a placeholder operator with identical\n",
            "functionality to allow for backward compatibility.\n",
            "  op_instances.append(_OperatorInstance(input_set, self, **kwargs))\n",
            "/usr/local/lib/python3.7/dist-packages/nvidia/dali/ops.py:627: DeprecationWarning: WARNING: `image_decoder` is now deprecated. Use `decoders.image` instead.\n",
            "In DALI 1.0 all decoders were moved into a dedicated :mod:`~nvidia.dali.fn.decoders`\n",
            "submodule and renamed to follow a common pattern. This is a placeholder operator with identical\n",
            "functionality to allow for backward compatibility.\n",
            "  op_instances.append(_OperatorInstance(input_set, self, **kwargs))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "torch.Size([32, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q629TILndltc"
      },
      "source": [
        "# type the torch size you checked above into the torch.ones parenthesis\n",
        "# to use this file in Image Similarity Search, you should have a gpu in your computer\n",
        "# if you don't have a gpu, run the next cell to get a cpu version .pt file\n",
        "\n",
        "with torch.no_grad():\n",
        "    x = torch.ones((32, 3, 256, 256)).cuda()  #typical looking datapoint = (1, 3, 256, 256))\n",
        "    traced_cell = torch.jit.trace(model, (x))\n",
        "torch.jit.save(traced_cell, \"UCMerced_simclr_minicnn32_50epochs.pt\") #change the file name as you want"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I019RDX8Zlde"
      },
      "source": [
        "# generate cpu version .pt file\n",
        "with torch.no_grad():\n",
        "    x = torch.ones((32, 3, 256, 256)).cpu()\n",
        "    traced_cell = torch.jit.trace(model.cpu(), (x))\n",
        "torch.jit.save(traced_cell, \"UCMerced_simclr_minicnn32_50epochs_cpu.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM_cVbsg2-6q"
      },
      "source": [
        "Now download the .pt file from the Colab notebook file directory to your computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGsRaVy_S5ZE"
      },
      "source": [
        "## 2-3. Check output embedding size of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f94Hl7iYOmqK"
      },
      "source": [
        "Embedding size is a required input in Image Similarity Search app so we should check the output embedding size of our SSL model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nF9mqLNL9WK",
        "outputId": "9dcbdb62-2a57-4945-faca-ae856e932dba"
      },
      "source": [
        "# check layers\n",
        "model = SIMCLR.SIMCLR.load_from_checkpoint('/content/models/SIMCLR_ssl.ckpt')\n",
        "model.eval()\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warmup\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "SIMCLR(\n",
              "  (projection): Projection(\n",
              "    (model): Sequential(\n",
              "      (0): Linear(in_features=32, out_features=128, bias=True)\n",
              "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=128, out_features=128, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (encoder): miniCNN(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (conv3): Conv2d(32, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (adaptive_pool): AdaptiveAvgPool2d(output_size=(16, 16))\n",
              "    (conv4): Conv2d(48, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR30viG0ML1r"
      },
      "source": [
        "In this demo, the output embedding is 32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWocbL-IcYbn"
      },
      "source": [
        "## 2-4. Run Image Similarity Search app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWQ0Xkr8ckLG"
      },
      "source": [
        "Now follow [this guide](https://github.com/spaceml-org/Curator-Unlabeled-Image-Search-Guide/blob/main/Single_Usage_Guide/Image_Similarity_Search.md) to set up and run the Image Similarity Search app on your computer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ThKMSCl8fE"
      },
      "source": [
        "<img width=\"854\" alt=\"ISS_screenshot\" src=\"https://user-images.githubusercontent.com/66165810/134059552-f64b23da-ecfe-40f7-aff5-5730dc9f2a78.PNG\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C-a7eBt_d5Y"
      },
      "source": [
        "#3. Active Learner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8PYTMUx29vd"
      },
      "source": [
        "## 3-1. Code setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhk4WIG02lS7",
        "outputId": "a3bc3e56-0df3-4238-83dd-673987ee82d8"
      },
      "source": [
        "%cd \"/content\"\n",
        "import os\n",
        "import shutil\n",
        "if os.path.exists('/content/Active-Labeller'):\n",
        "  shutil.rmtree('/content/Active-Labeller')\n",
        "\n",
        "!git clone https://github.com/spaceml-org/Active-Labeller.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Active-Labeller'...\n",
            "remote: Enumerating objects: 2031, done.\u001b[K\n",
            "remote: Counting objects: 100% (2031/2031), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1474/1474), done.\u001b[K\n",
            "remote: Total 2031 (delta 615), reused 1889 (delta 534), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2031/2031), 24.02 MiB | 26.37 MiB/s, done.\n",
            "Resolving deltas: 100% (615/615), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oaHLXIL3glG"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG,filename='/content/app.log', filemode='a', format='%(asctime)s - %(levelname)-8s - %(funcName)-15s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dGzvbrO3CPJ"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/Active-Labeller\")\n",
        "sys.path.insert(0, \"/content/Active-Labeller/ActiveLabeler-main\")\n",
        "sys.path.insert(0, \"/content/Active-Labeller/ActiveLabeler-main/Self-Supervised-Learner\")\n",
        "sys.path.insert(0, \"/content/Active-Labeller/ActiveLabeler-main/ActiveLabelerModels\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfKX-ynx3Qyo"
      },
      "source": [
        "import random \n",
        "random.seed(seed)\n",
        "import numpy as np \n",
        "np.random.seed(seed) #TODO random in code uses seed in each line ? \n",
        "\n",
        "config_path = \"/content/Active-Labeller/pipeline_config.yml\"\n",
        "from pipeline import Pipeline\n",
        "pipeline = Pipeline(config_path,\"airplane\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS9RTjaLElDk"
      },
      "source": [
        "Access Swipe labeler at the following link:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "Wg2HWlss3SXA",
        "outputId": "96650e53-9ec1-4fd2-bb12-5b73c9fe3520"
      },
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://wgooh68b6oj-496ff2e9c6d22116-5000-colab.googleusercontent.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbcjD3FK34xL"
      },
      "source": [
        "## 3-2. Run Active Labeler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJoBCYyddgKC"
      },
      "source": [
        "Run the cell below and open the above link to label the images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "6c4f50446d914ea6801a7311a3a0c541"
          ]
        },
        "id": "y1fxpdUB3oKt",
        "outputId": "5f4bf5ef-10d2-4ccc-8ac8-7378e7e34e0d"
      },
      "source": [
        "import random \n",
        "random.seed(100)\n",
        "import numpy as np \n",
        "np.random.seed(100)\n",
        "\n",
        "pipeline.main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c4f50446d914ea6801a7311a3a0c541",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/Active-Labeller/pipeline.py:169: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  im = torch.Tensor(im).unsqueeze(0).cuda()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Got embeddings. Embedding Shape: torch.Size([2100, 32])\n",
            "Annoy file stored at  /content/runtime/NN_local/annoy_file.ann\n",
            "\n",
            "----- iteration: 1\n",
            "Enter n closest\n",
            "\n",
            " 10 images to label.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vay6JmTs16Yy"
      },
      "source": [
        "model = torch.load(\"/content/final_model.ckpt\")\n",
        "\n",
        "def to_tensor(pil):\n",
        "            return torch.tensor(np.array(pil)).permute(2, 0, 1).float()\n",
        "\n",
        "t = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.Lambda(to_tensor)\n",
        "        ])\n",
        "\n",
        "dataset = ImageFolder(\"/content/Dataset\", transform=t)\n",
        "img_paths = [i[0] for i in dataset.imgs]\n",
        "\n",
        "unlabeled_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    bs = 128\n",
        "    if len(dataset) < bs:\n",
        "        bs = 1\n",
        "    loader = DataLoader(dataset, batch_size=bs, shuffle=False)\n",
        "    for batch in tqdm(loader):\n",
        "        x = batch[0].cuda()\n",
        "        feats = model.encoder(x)[-1]\n",
        "        feats = feats.view(feats.size(0), -1)\n",
        "        predictions = model.linear_model(feats)\n",
        "        unlabeled_predictions.extend(predictions.detach().cpu().numpy())\n",
        "unlabeled_predictions = [1 if x > 0.5 else 0 for x in unlabeled_predictions]\n",
        "print(unlabeled_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB3mlAEOk3mz"
      },
      "source": [
        "os.mkdir(\"/content/AL_Dataset\")\n",
        "os.mkdir(\"/content/AL_Dataset/Positive\")\n",
        "os.mkdir(\"/content/AL_Dataset/Negative\")\n",
        "\n",
        "\n",
        "for i in range(len(img_paths)):\n",
        "    if unlabeled_predictions[i] == 0:\n",
        "        target = os.path.join(\"/content/AL_Dataset/Negative\", img_paths[i].split(\"/\")[-1])\n",
        "        shutil.move(img_paths[i], target)\n",
        "    else:\n",
        "        target = os.path.join(\"/content/AL_Dataset/Positive\", img_paths[i].split(\"/\")[-1])\n",
        "        shutil.move(img_paths[i], target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9JZTMQdKvfX"
      },
      "source": [
        "print(len(list(paths.list_images('/content/AL_Dataset/Positive'))))\n",
        "print(len(list(paths.list_images('/content/AL_Dataset/Negative'))))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}